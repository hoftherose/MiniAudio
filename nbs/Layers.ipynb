{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layers\n",
    "> Important Model Layers for audio related tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai2.basics import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Invertible Conv Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Invertible1x1Conv(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    The layer outputs both the convolution, and the log determinant\n",
    "    of its weight matrix.  If reverse=True it does convolution with\n",
    "    inverse\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, c):\n",
    "        super(Invertible1x1Conv, self).__init__()\n",
    "        self.conv = torch.nn.Conv1d(c, c, kernel_size=1, stride=1, padding=0,\n",
    "                                    bias=False)\n",
    "\n",
    "        # Sample a random orthonormal matrix to initialize weights\n",
    "        W = torch.qr(torch.FloatTensor(c, c).normal_())[0]\n",
    "\n",
    "        # Ensure determinant is 1.0 not -1.0\n",
    "        if torch.det(W) < 0:\n",
    "            W[:, 0] = -1 * W[:, 0]\n",
    "        W = W.view(c, c, 1)\n",
    "        self.conv.weight.data = W\n",
    "\n",
    "    def forward(self, z, reverse=False):\n",
    "        # shape\n",
    "        batch_size, group_size, n_of_groups = z.size()\n",
    "\n",
    "        W = self.conv.weight.squeeze()\n",
    "\n",
    "        if reverse:\n",
    "            if not hasattr(self, 'W_inverse'):\n",
    "                # Reverse computation\n",
    "                W_inverse = W.inverse()\n",
    "                W_inverse = Variable(W_inverse[..., None])\n",
    "                if z.type() == 'torch.cuda.HalfTensor' or z.type() == 'torch.HalfTensor':\n",
    "                    W_inverse = W_inverse.half()\n",
    "                self.W_inverse = W_inverse\n",
    "            z = F.conv1d(z, self.W_inverse, bias=None, stride=1, padding=0)\n",
    "            return z\n",
    "        else:\n",
    "            # Forward computation\n",
    "            log_det_W = torch.logdet(W)\n",
    "            log_det_W = batch_size * n_of_groups * log_det_W\n",
    "            if z.dtype == torch.float16:\n",
    "                z = self.conv(z.float()).half()\n",
    "            else:\n",
    "                z = self.conv(z)\n",
    "            return z, log_det_W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_layer = Invertible1x1Conv(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.tensor([[[1.],[2.],[3.]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-3.0882],\n",
       "          [-1.8305],\n",
       "          [-1.0546]]], grad_fn=<SqueezeBackward1>),\n",
       " tensor(1.2899e-07, grad_fn=<MulBackward0>))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = inv_layer(inp)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 1])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Conv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dilated Conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dila_conv = nn.Conv1d(3, 1, kernel_size=2, dilation=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.tensor([[[1.,2.,3.,4.],[1.,2.,3.,4.],[1.,2.,3.,4.]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0838, 1.2428]]], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = dila_conv(inp)\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UpSampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MiniAudio.data import Clipper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "signal = np.random.rand(44100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(84, 87)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "librosa.cqt(signal, 44100).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(168, 173)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "librosa.cqt(signal, 44100, hop_length=256, n_bins=168, bins_per_octave=24).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.rand(1, 168, 173)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upsample = torch.nn.ConvTranspose1d(168,168,1024, stride=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 168, 45056])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upsample(inp).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WaveNet Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@delegates(torch.nn.Conv1d)\n",
    "def WeightedConv1d(in_channels, out_channels, kernel_size, **kwargs):\n",
    "    layer = nn.Conv1d(in_channels, out_channels, kernel_size, **kwargs)\n",
    "    return torch.nn.utils.weight_norm(layer, name='weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaveNetLayer(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Traditional WaveNet Layer which outputs both the residual value and the output.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_mel_channels, kernel_size, dilation, padding):\n",
    "        super(WaveNetLayer, self).__init__()\n",
    "        self.SigmDilation = WeightedConv1d(n_mel_channels, 2*n_mel_channels, kernel_size,\n",
    "                                           dilation=dilation, padding=padding)\n",
    "        self.TanhDilation = WeightedConv1d(n_mel_channels, 2*n_mel_channels, kernel_size,\n",
    "                                           dilation=dilation, padding=padding)\n",
    "        self.ResLinear = torch.nn.Conv1d(2*n_mel_channels,n_mel_channels, 1, stride=1)\n",
    "        self.OutLinear = torch.nn.Conv1d(2*n_mel_channels,n_mel_channels, 1, stride=1)\n",
    "    def forward(self, x):\n",
    "        sigm_output = torch.nn.Sigmoid()(self.SigmDilation(x))\n",
    "        tanh_output = torch.nn.Tanh()(self.TanhDilation(x))\n",
    "        prod = torch.mul(sigm_output, tanh_output)\n",
    "        out = self.OutLinear(prod)\n",
    "        res = self.ResLinear(prod)+x\n",
    "        return out, res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaveNet(torch.nn.Module):\n",
    "    def __init__(self, n_mel_channels, precision, n_layers, kernel_size):\n",
    "        super(WaveNet, self).__init__()\n",
    "        store_attr(self, 'n_mel_channels, precision, n_layers, kernel_size')\n",
    "        self.upsample = torch.nn.ConvTranspose1d(n_mel_channels,n_mel_channels,1024, stride=256)\n",
    "        self.WaveNetLayers = torch.nn.ModuleList()\n",
    "        for i in range(self.n_layers):\n",
    "            dilation = 2 ** i\n",
    "            padding = int((kernel_size * dilation - dilation) / 2)\n",
    "            self.WaveNetLayers.append(WaveNetLayer(n_mel_channels, kernel_size, dilation, padding))\n",
    "        self.end = torch.nn.Sequential(torch.nn.ReLU(),torch.nn.Conv1d(n_mel_channels,n_mel_channels//2, 1),\n",
    "                                       torch.nn.ReLU(),torch.nn.Conv1d(n_mel_channels//2,n_mel_channels//4, 1),\n",
    "                                       torch.nn.ReLU(),torch.nn.Conv1d(n_mel_channels//4,precision, 1),\n",
    "                                       torch.nn.Softmax(dim=1))\n",
    "\n",
    "    def forward(self, spec):\n",
    "        x = self.upsample(spec)\n",
    "        for i in range(self.n_layers):\n",
    "            out, x = self.WaveNetLayers[i](x)\n",
    "            if i == 0: output = out\n",
    "            else:      output = out+output\n",
    "        print(output.shape)\n",
    "        return self.end(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wavenet = WaveNet(n_mel_channels=168, precision=256, n_layers=1, kernel_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 168, 221440])\n"
     ]
    }
   ],
   "source": [
    "ans = wavenet(torch.rand(1, 168, 862))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 221440])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
